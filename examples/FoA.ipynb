{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from diskcache import Cache\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "from dataclasses import dataclass\n",
    "from together import AsyncTogether\n",
    "\n",
    "from cachesaver.typedefs import Request\n",
    "from cachesaver.pipelines import OnlineAPI\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.tasks import EnvironmentBasic\n",
    "from src.frameworks import FrameworkFoA\n",
    "from src.agents import AgentLLM\n",
    "from src.models import OnlineLLM\n",
    "from src.utils import tokens2cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose task\n",
    "task = \"hotpotqa\" # \"hotpotqa\" or \"game24\"\n",
    "\n",
    "# Config\n",
    "config = OmegaConf.load(f'../configs/{task}/config_foa_{task}.yaml')\n",
    "\n",
    "# Environment\n",
    "env = EnvironmentBasic.create(task=task, data_path=f\"../datasets/dataset_{task}.csv.gz\")\n",
    "\n",
    "# Cache\n",
    "cache = Cache(f\"../caches/{task}\")\n",
    "\n",
    "# LLM Client and Model\n",
    "client = AsyncTogether(api_key=os.environ.get('TOGETHER_API_KEY_PERS'))\n",
    "model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "model = OnlineLLM(client, model=model_name)\n",
    "\n",
    "# CacheSaver API\n",
    "api = OnlineAPI(\n",
    "    model=model,\n",
    "    cache=cache,\n",
    "    batch_size=30,\n",
    "    timeout=0.5,\n",
    ")\n",
    "\n",
    "# Agent\n",
    "agent = AgentLLM(api=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic FoA operations test\n",
    "\n",
    "state = env.reset(0)\n",
    "print(f\"{state=}\\n\")\n",
    "\n",
    "new_state = await agent.foa_step(\n",
    "    state=state, \n",
    "    environment=env,\n",
    "    namespace=\"0\",\n",
    "    request_id=\"0\",\n",
    "    cache=None,\n",
    "    config=config.api.parameters\n",
    "    )\n",
    "print(f\"{new_state=}\\n\")\n",
    "\n",
    "value = await agent.evaluate(\n",
    "    state=new_state,\n",
    "    environment=env,\n",
    "    n=3,\n",
    "    namespace=\"0\",\n",
    "    request_id=\"1\",\n",
    "    cache=None,\n",
    "    config=config.api.parameters\n",
    "    )\n",
    "print(f\"{value=}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Framework\n",
    "fw = FrameworkFoA(config, agent, env)\n",
    "\n",
    "for seed in range(config.run.repeats):\n",
    "    log = {}\n",
    "    step_cache = None\n",
    "    value_cache = {}\n",
    "    \n",
    "\n",
    "    # Get the puzzle indexes for the current set\n",
    "    set = config.run.set\n",
    "    puzzle_idxs = env.data.get_set_idxs(set)[:config.run.debugging]\n",
    "\n",
    "    # Run the puzzles in parallel\n",
    "    puzzle_coroutines = [\n",
    "        fw.run(\n",
    "            puzzle_idx=puzzle_idx,\n",
    "            namespace=str(puzzle_idx),\n",
    "            seed=seed,\n",
    "            value_cache=value_cache,\n",
    "            step_cache=step_cache,\n",
    "        )\n",
    "        for puzzle_idx in puzzle_idxs\n",
    "    ]\n",
    "    results = await asyncio.gather(*puzzle_coroutines)\n",
    "    states, verifications,logs = map(list, zip(*results))\n",
    "\n",
    "    # Compute quality\n",
    "    success = [any(v.finished and v.correct for v in vs) for vs in verifications]\n",
    "    accuracy = sum(success) / len(success)\n",
    "\n",
    "    # Merge the logs\n",
    "    for l in logs:\n",
    "        log.update(l)\n",
    "    \n",
    "    # Saving additional info in the log\n",
    "    log[\"Info\"] = {}\n",
    "    \n",
    "    log[\"Info\"][\"Cost\"] = {\n",
    "            \"total\": tokens2cost(agent.tokens[\"total\"], model_name),\n",
    "            \"cached\": tokens2cost(agent.tokens[\"cached\"], model_name),\n",
    "            \"generated\": tokens2cost(agent.tokens[\"generated\"], model_name),\n",
    "        }\n",
    "    \n",
    "    config = OmegaConf.to_container(config, resolve=True)\n",
    "    log[\"Info\"][\"LLM\"] = {\"model\": config[\"api\"][\"model\"], \"parameters\": config[\"api\"][\"parameters\"]}\n",
    "    log[\"Info\"][\"Framework\"] = config[\"framework\"]\n",
    "    log[\"Task\"] = config[\"task\"]\n",
    "    log[\"Run\"] = {\"seed\": seed, \"set\": config[\"run\"][\"set\"], \"debugging\": config[\"run\"][\"debugging\"]}\n",
    "    log[\"Quality\"] = {\"accuracy\": accuracy, \"success\": success}\n",
    "\n",
    "    # Save the results of the trial\n",
    "    log_dir = os.path.join(os.getcwd().split(\"cachesaver\")[0] + \"cachesaver\", config[\"logs\"][\"log_dir\"])\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = os.path.join(log_dir, f\"{config['logs']['log_name']}_{seed}.json\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        json.dump(log, f, indent=4)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cachesaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
