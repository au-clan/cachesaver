{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CacheSaver Tutorial\n",
    "\n",
    "**CacheSaver** is a caching library for LLM inference that minimizes API costs through intelligent caching, request deduplication, and batching. It works as a **drop-in replacement** for your existing LLM client — same API, transparent caching.\n",
    "\n",
    "This tutorial walks through practical scenarios where CacheSaver saves you time and money:\n",
    "1. **Quickstart** — swap in CacheSaver with one import change\n",
    "2. **Reproducibility** — get identical results across runs\n",
    "3. **Error recovery** — resume experiments without re-paying for completed work\n",
    "4. **Iterative development** — modify algorithms without re-running unchanged steps\n",
    "5. **Parallelism** — speed up experiments with async execution\n",
    "6. **Complex applications** — ReAct agents, Tree-of-Thought, and RAG pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cachesaver openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.CRITICAL)\n",
    "\n",
    "import shutil, os, time, asyncio, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. LLM Inference: Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. OpenAI — without vs. with CacheSaver\n",
    "\n",
    "First, a standard OpenAI call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()  # uses OPENAI_API_KEY env var\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}],\n",
    ")\n",
    "print(\"OpenAI SDK:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same call with CacheSaver — just change the import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cache for a fresh demo\n",
    "shutil.rmtree(\"./cache/quickstart\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(namespace=\"quickstart\", cachedir=\"./cache/quickstart\")\n",
    "\n",
    "# First call — hits the API\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}],\n",
    ")\n",
    "print(\"CacheSaver (1st call):\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(namespace=\"quickstart\", cachedir=\"./cache/quickstart\")\n",
    "\n",
    "# Second call — same prompt, returned from cache (no API cost)\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}],\n",
    ")\n",
    "print(\"CacheSaver (2nd call, cached):\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second call returns instantly with the exact same result — no API request was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. HuggingFace Transformers — without vs. with CacheSaver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CacheSaver also wraps local models. With standard `transformers`:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "With CacheSaver (same result, with automatic caching and batching):\n",
    "\n",
    "```python\n",
    "from cachesaver.models.transformers import AsyncHFTransformers\n",
    "\n",
    "client = AsyncHFTransformers(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    namespace=\"hf_demo\",\n",
    "    cachedir=\"./cache/hf\",\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "# First call — runs GPU inference\n",
    "response = await client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}],\n",
    "    max_new_tokens=20,\n",
    ")\n",
    "print(\"First call:\", response)\n",
    "\n",
    "# Second call — returned from cache, no GPU needed\n",
    "response = await client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}],\n",
    "    max_new_tokens=20,\n",
    ")\n",
    "print(\"Cached:\", response)\n",
    "```\n",
    "\n",
    "> **Note:** The HF Transformers sections are shown as code blocks because they require a GPU and the `transformers` package (`pip install cachesaver[transformers]`). The rest of this tutorial uses the OpenAI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CacheSaver: Making everything fully reproducible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Reproducibility\n",
    "\n",
    "Run an experiment, then re-run it from scratch — CacheSaver guarantees identical results from cache.\n",
    "\n",
    "**Namespaces** track which cached responses have been used, so even prompts that appear multiple times get the correct response on replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/reproducibility\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "sentences = [\n",
    "    \"I love this product, it works great!\",\n",
    "    \"Terrible experience, would not recommend.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Absolutely fantastic, exceeded my expectations!\",\n",
    "    \"Worst purchase I've ever made.\",\n",
    "]\n",
    "\n",
    "async def classify_sentiment(sentences, namespace):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/reproducibility\")\n",
    "    results = []\n",
    "    for s in sentences:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Classify the sentiment as positive/negative/neutral (one word only): {s}\"}],\n",
    "        )\n",
    "        results.append(response.choices[0].message.content)\n",
    "    return results\n",
    "\n",
    "# Run 1 — calls the API\n",
    "run1 = await classify_sentiment(sentences, namespace=\"experiment_v1\")\n",
    "print(\"Run 1:\", run1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2 — same namespace, same results from cache (simulates a fresh restart)\n",
    "run2 = await classify_sentiment(sentences, namespace=\"experiment_v1\")\n",
    "print(\"Run 2:\", run2)\n",
    "print(\"Identical?\", run1 == run2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both runs produce identical results. The second run costs nothing — all responses came from cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Repeat experiment after an error\n",
    "\n",
    "Imagine processing 10 items and hitting an error on item 7. Without CacheSaver, you'd re-pay for items 1–6 on retry. With CacheSaver, only items 7–10 hit the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/error_recovery\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "topics = [\n",
    "    \"Climate change\",\n",
    "    \"Artificial intelligence\",\n",
    "    \"Space exploration\",\n",
    "    \"Renewable energy\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Mental health awareness\",\n",
    "    \"Quantum computing\",\n",
    "    \"Sustainable agriculture\",\n",
    "    \"The future of work\",\n",
    "    \"Robotics\"\n",
    "]\n",
    "\n",
    "items = [f\"Write a one-sentence summary of topic {t}.\" for t in topics]\n",
    "\n",
    "async def process_items(items, namespace, simulate_error_at=None):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/error_recovery\")\n",
    "    results = []\n",
    "    for i, item in enumerate(items):\n",
    "        if simulate_error_at is not None and i == simulate_error_at:\n",
    "            raise RuntimeError(f\"Simulated error at item {i+1}!\")\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": item}],\n",
    "        )\n",
    "        text = response.choices[0].message.content\n",
    "        results.append(text)\n",
    "        print(f\"  Item {i+1}: {text}\")\n",
    "    return results\n",
    "\n",
    "# First attempt — crashes on item 6 (0-indexed)\n",
    "print(\"=== Attempt 1 (will fail at item 6) ===\")\n",
    "try:\n",
    "    results = await process_items(items, namespace=\"error_demo\", simulate_error_at=6)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Items 1-7 are cached, items 7-10 need to be re-run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second attempt — no error this time\n",
    "# Items 0-5 come from cache instantly, only 6-9 call the API\n",
    "print(\"=== Attempt 2 (no error) ===\")\n",
    "results = await process_items(items, namespace=\"error_demo\")\n",
    "print(f\"\\nAll {len(results)} items completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Repeat experiment when you forgot to add logs\n",
    "\n",
    "You ran an experiment but forgot to log intermediate results. With CacheSaver, just add the logging and re-run — the cached responses are reused, so you get the exact same results with your new logging, at zero extra cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/logging_demo\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "questions = [\n",
    "    \"What is photosynthesis?\",\n",
    "    \"Explain gravity in one sentence.\",\n",
    "    \"What causes rain?\",\n",
    "]\n",
    "\n",
    "# Version 1: no logging, just collect results\n",
    "async def run_without_logging(questions, namespace):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/logging_demo\")\n",
    "    results = []\n",
    "    for q in questions:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": q}],\n",
    "        )\n",
    "        results.append(response.choices[0].message.content)\n",
    "    return results\n",
    "\n",
    "print(\"=== Run without logging ===\")\n",
    "results_v1 = await run_without_logging(questions, namespace=\"logging_exp\")\n",
    "for r in results_v1:\n",
    "    print(f\"  {r[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: oops, we forgot to log! Add logging and re-run.\n",
    "# Same responses come from cache — no extra API cost.\n",
    "async def run_with_logging(questions, namespace):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/logging_demo\")\n",
    "    log = []\n",
    "    results = []\n",
    "    for q in questions:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": q}],\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        results.append(answer)\n",
    "        log.append({\"question\": q, \"answer\": answer, \"model\": \"gpt-4.1-nano\"})\n",
    "    return results, log\n",
    "\n",
    "print(\"=== Re-run with logging (all cached) ===\")\n",
    "results_v2, log = await run_with_logging(questions, namespace=\"logging_exp\")\n",
    "print(\"Results identical?\", results_v1 == results_v2)\n",
    "print(\"\\nLog entries:\")\n",
    "for entry in log:\n",
    "    print(f\"  Q: {entry['question']}\")\n",
    "    print(f\"  A: {entry['answer'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. CacheSaver: Making developping easier and stress-free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Algorithm A → Algorithm A + modifications\n",
    "\n",
    "You have a pipeline (Algorithm A) that summarizes articles and extracts keywords. You then modify it (Algorithm A') to also add sentiment analysis. CacheSaver ensures the summarization step is cached — only the new sentiment calls hit the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/algorithm\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "articles = [\n",
    "    \"Scientists discover high-temperature superconductor that works at room temperature, potentially revolutionizing energy.\",\n",
    "    \"New study finds that remote workers report higher job satisfaction but struggle with collaboration.\",\n",
    "    \"Global chip shortage eases as major fabs increase production capacity by 40 percent.\",\n",
    "    \"Research team develops biodegradable plastic from seaweed that decomposes in 6 weeks.\",\n",
    "    \"City implements congestion pricing, reducing downtown traffic by 25 percent in first month.\",\n",
    "]\n",
    "\n",
    "# Algorithm A: summarize + extract keywords\n",
    "async def algorithm_a(articles, namespace):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/algorithm\")\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Step 1: Summarize\n",
    "        summary_resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Summarize in one sentence: {article}\"}],\n",
    "        )\n",
    "        summary = summary_resp.choices[0].message.content\n",
    "\n",
    "        # Step 2: Extract keywords\n",
    "        kw_resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Extract 3 keywords (comma-separated): {article}\"}],\n",
    "        )\n",
    "        keywords = kw_resp.choices[0].message.content\n",
    "\n",
    "        results.append({\"summary\": summary, \"keywords\": keywords})\n",
    "    return results\n",
    "\n",
    "print(\"=== Algorithm A ===\")\n",
    "results_a = await algorithm_a(articles, namespace=\"algo_exp\")\n",
    "for i, r in enumerate(results_a):\n",
    "    print(f\"\\nArticle {i+1}:\")\n",
    "    print(f\"  Summary:  {r['summary'][:80]}...\")\n",
    "    print(f\"  Keywords: {r['keywords']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm A': add sentiment analysis after summarization\n",
    "# Summarize and keyword steps are CACHED — only sentiment calls hit the API\n",
    "async def algorithm_a_prime(articles, namespace):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/algorithm\")\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Step 1: Summarize (CACHED)\n",
    "        summary_resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Summarize in one sentence: {article}\"}],\n",
    "        )\n",
    "        summary = summary_resp.choices[0].message.content\n",
    "\n",
    "        # Step 2: Extract keywords (CACHED)\n",
    "        kw_resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Extract 3 keywords (comma-separated): {article}\"}],\n",
    "        )\n",
    "        keywords = kw_resp.choices[0].message.content\n",
    "\n",
    "        # Step 3: Sentiment analysis (NEW — hits API)\n",
    "        sent_resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"What is the sentiment of this text? Answer positive/negative/neutral: {article}\"}],\n",
    "        )\n",
    "        sentiment = sent_resp.choices[0].message.content\n",
    "\n",
    "        results.append({\"summary\": summary, \"keywords\": keywords, \"sentiment\": sentiment})\n",
    "    return results\n",
    "\n",
    "print(\"=== Algorithm A' (modified — only sentiment calls are new) ===\")\n",
    "results_a_prime = await algorithm_a_prime(articles, namespace=\"algo_exp\")\n",
    "for i, r in enumerate(results_a_prime):\n",
    "    print(f\"\\nArticle {i+1}:\")\n",
    "    print(f\"  Summary:   {r['summary'][:80]}...\")\n",
    "    print(f\"  Keywords:  {r['keywords']}\")\n",
    "    print(f\"  Sentiment: {r['sentiment']}\")\n",
    "\n",
    "# Verify that the summarization and keyword results are identical\n",
    "for i in range(len(articles)):\n",
    "    assert results_a[i][\"summary\"] == results_a_prime[i][\"summary\"]\n",
    "    assert results_a[i][\"keywords\"] == results_a_prime[i][\"keywords\"]\n",
    "print(\"\\nSummary and keyword results are identical to Algorithm A.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Parallelism: Async Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. CacheSaver is fully asynchronous\n",
    "\n",
    "Use `asyncio.gather` to run multiple requests concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/parallel\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(namespace=\"parallel_demo\", cachedir=\"./cache/parallel\")\n",
    "\n",
    "prompts = [\n",
    "    \"What is the largest ocean?\",\n",
    "    \"What is the smallest country?\",\n",
    "    \"What is the longest river?\",\n",
    "    \"What is the tallest mountain?\",\n",
    "    \"What is the deepest lake?\",\n",
    "]\n",
    "\n",
    "async def ask(prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} Answer in one word.\"}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Sequential\n",
    "t0 = time.time()\n",
    "sequential_results = []\n",
    "for p in prompts:\n",
    "    sequential_results.append(await ask(p))\n",
    "seq_time = time.time() - t0\n",
    "print(f\"Sequential: {seq_time:.2f}s\")\n",
    "print(\"Results:\", sequential_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask2(prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{prompt} Answer in one word.\"}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Parallel\n",
    "t0 = time.time()\n",
    "parallel_results = await asyncio.gather(*[ask2(p) for p in prompts])\n",
    "par_time = time.time() - t0\n",
    "print(f\"Parallel:   {par_time:.2f}s\")\n",
    "print(\"Results:\", list(parallel_results))\n",
    "print(f\"\\nSpeedup: {seq_time / par_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Deterministic ordering with async agents\n",
    "\n",
    "A common pattern: make one LLM call requesting `n=5` outputs, then dispatch each output to a separate \"agent\" (another LLM call) for further processing. These 5 agents run concurrently with `asyncio.gather`.\n",
    "\n",
    "**The problem:** async tasks finish in unpredictable order. If you build your own cache by storing results in completion order, replaying gives agents the wrong outputs — the mapping is scrambled.\n",
    "\n",
    "**CacheSaver solves this** because it caches based on the *content* of each request, not the order they arrive. Every replay is deterministic regardless of which agent finishes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shutil.rmtree(\"./cache/ordering\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(namespace=\"ordering_demo\", cachedir=\"./cache/ordering\")\n",
    "\n",
    "starters = [\"As the moon cast a silvery glow, the tiny owl unintentionally unlocked a secret world hidden within the ancient forest’s shadows.\"] * 5\n",
    "print(\"Generated 5 story starters:\")\n",
    "for i, s in enumerate(starters):\n",
    "    print(f\"  Agent {i}: {s}\")\n",
    "\n",
    "# Step 2: Dispatch each starter to a parallel \"agent\" that continues the story.\n",
    "# Each agent adds a random sleep to simulate variable processing time,\n",
    "# so the completion order is non-deterministic.\n",
    "async def agent(agent_id, starter):\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Continue this story in one sentence: {starter}\"},\n",
    "        ],\n",
    "    )\n",
    "    continuation = resp.choices[0].message.content\n",
    "    return continuation\n",
    "\n",
    "continuations_run1 = await asyncio.gather(*[agent(i, s) for i, s in enumerate(starters)])\n",
    "\n",
    "print(\"\\nResults (agent → continuation):\")\n",
    "for i, (starter, cont) in enumerate(zip(starters, continuations_run1)):\n",
    "    print(f\"  Agent {i}: {cont[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2 — replay from cache with different random delays.\n",
    "# Despite agents finishing in a completely different order, each agent\n",
    "# gets the exact same continuation as before.\n",
    "print(\"=== Run 2 (cached) — different agent completion order, same results ===\")\n",
    "client_replay = AsyncOpenAI(namespace=\"ordering_demo\", cachedir=\"./cache/ordering\")\n",
    "\n",
    "async def agent_replay(agent_id, starter):\n",
    "    resp = await client_replay.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Continue this story in one sentence: {starter}\"},\n",
    "        ],\n",
    "    )\n",
    "    continuation = resp.choices[0].message.content\n",
    "    return continuation\n",
    "\n",
    "random.seed(None)  # new random delays → different completion order\n",
    "continuations_run2 = await asyncio.gather(*[agent_replay(i, s) for i, s in enumerate(starters)])\n",
    "\n",
    "print(\"\\nResults (agent → continuation):\")\n",
    "for i, cont in enumerate(continuations_run2):\n",
    "    print(f\"  Agent {i}: {cont[:80]}...\")\n",
    "\n",
    "print(f\"Same continuations? {continuations_run1 == list(continuations_run2)}\")\n",
    "print(\"\\nEach agent got the same output despite their asynchronicity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. CacheSaver makes experiments faster\n",
    "\n",
    "Run a batch of 20 requests: first sequentially, then in parallel, then from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/batch_speed\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "batch_prompts = [f\"Give me a fun fact about the number {i}. One sentence only.\" for i in range(1, 11)]\n",
    "\n",
    "# Sequential\n",
    "client = AsyncOpenAI(namespace=\"batch_seq\", cachedir=\"./cache/batch_speed\")\n",
    "\n",
    "t0 = time.time()\n",
    "seq_results = []\n",
    "for p in batch_prompts:\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": p}],\n",
    "    )\n",
    "    seq_results.append(resp.choices[0].message.content)\n",
    "seq_time = time.time() - t0\n",
    "print(f\"Sequential (10 requests): {seq_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel with asyncio.gather\n",
    "\n",
    "async def fetch(prompt):\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "t0 = time.time()\n",
    "par_results = await asyncio.gather(*[fetch(p) for p in batch_prompts])\n",
    "par_time = time.time() - t0\n",
    "print(f\"Parallel (20 requests):   {par_time:.2f}s  ({seq_time / par_time:.1f}x faster)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached — near-instant\n",
    "client = AsyncOpenAI(namespace=\"batch_par\", cachedir=\"./cache/batch_speed\")\n",
    "\n",
    "async def fetch_cached(prompt):\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "t0 = time.time()\n",
    "cached_results = await asyncio.gather(*[fetch_cached(p) for p in batch_prompts])\n",
    "cache_time = time.time() - t0\n",
    "print(f\"Cached (20 requests):     {cache_time:.2f}s\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Sequential: {seq_time:.2f}s\")\n",
    "print(f\"  Parallel:   {par_time:.2f}s\")\n",
    "print(f\"  Cached:     {cache_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Complex AI Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. ReAct Agent with CacheSaver\n",
    "\n",
    "A **ReAct** agent interleaves thinking and acting: it reasons about what to do, calls a tool, observes the result, and repeats.\n",
    "\n",
    "We implement a simple ReAct loop with two mock tools and use CacheSaver so re-running the agent is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/react\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "# Define simple tools\n",
    "def lookup_population(country):\n",
    "    \"\"\"Look up the population of a country (mock data).\"\"\"\n",
    "    data = {\n",
    "        \"france\": 67_390_000,\n",
    "        \"belgium\": 11_590_000,\n",
    "        \"germany\": 83_200_000,\n",
    "        \"japan\": 125_700_000,\n",
    "        \"brazil\": 214_000_000,\n",
    "    }\n",
    "    return data.get(country.lower().strip(), \"Unknown country\")\n",
    "\n",
    "def calculator(expression):\n",
    "    \"\"\"Evaluate a simple math expression.\"\"\"\n",
    "    try:\n",
    "        return str(round(eval(expression, {\"__builtins__\": {}}), 2))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "tools = {\"lookup_population\": lookup_population, \"calculator\": calculator}\n",
    "\n",
    "REACT_SYSTEM = \"\"\"You are a helpful assistant that can use tools to answer questions.\n",
    "\n",
    "Available tools:\n",
    "- lookup_population(country): Returns the population of a country.\n",
    "- calculator(expression): Evaluates a math expression.\n",
    "\n",
    "To use a tool, respond EXACTLY in this format:\n",
    "Thought: <your reasoning>\n",
    "Action: <tool_name>(<argument>)\n",
    "\n",
    "When you have the final answer, respond EXACTLY:\n",
    "Thought: <your reasoning>\n",
    "Answer: <final answer>\n",
    "\n",
    "Always start with a Thought.\"\"\"\n",
    "\n",
    "async def react_agent(client, question):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": REACT_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    for step in range(6):  # max steps\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        print(f\"\\n--- Step {step + 1} ---\")\n",
    "        print(reply)\n",
    "\n",
    "        # Check for final answer\n",
    "        if \"Answer:\" in reply:\n",
    "            answer = reply.split(\"Answer:\")[-1].strip()\n",
    "            return answer\n",
    "\n",
    "        # Parse action\n",
    "        if \"Action:\" in reply:\n",
    "            action_line = reply.split(\"Action:\")[-1].strip()\n",
    "            # Parse tool_name(argument)\n",
    "            paren_idx = action_line.index(\"(\")\n",
    "            tool_name = action_line[:paren_idx].strip()\n",
    "            argument = action_line[paren_idx+1:].rstrip(\")\")\n",
    "            argument = argument.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "            if tool_name in tools:\n",
    "                observation = str(tools[tool_name](argument))\n",
    "            else:\n",
    "                observation = f\"Unknown tool: {tool_name}\"\n",
    "\n",
    "            print(f\"Observation: {observation}\")\n",
    "            messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Please continue. Use a tool or provide a final Answer.\"})\n",
    "\n",
    "    return \"Max steps reached\"\n",
    "\n",
    "print(\"=== ReAct Agent — Run 1 ===\")\n",
    "client = AsyncOpenAI(namespace=\"react_demo\", cachedir=\"./cache/react\")\n",
    "answer = await react_agent(\n",
    "    client,\n",
    "    \"What is the population of France divided by the population of Belgium? Give the ratio.\",\n",
    ")\n",
    "print(f\"\\nFinal answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run — all LLM calls cached, deterministic replay\n",
    "print(\"=== ReAct Agent — Run 2 (cached) ===\")\n",
    "answer2 = await react_agent(\n",
    "    client,\n",
    "    \"What is the population of France divided by the population of Belgium? Give the ratio.\"\n",
    ")\n",
    "print(f\"\\nFinal answer: {answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Tree-of-Thought BFS with CacheSaver\n",
    "\n",
    "**Tree-of-Thought (ToT)** explores multiple reasoning paths in parallel. At each depth:\n",
    "1. Generate N candidate \"thoughts\" for each current state\n",
    "2. Evaluate/score each candidate\n",
    "3. Keep the top-K and expand further\n",
    "\n",
    "CacheSaver caches all LLM calls, so re-running the search is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/tot\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "PROBLEM = \"\"\"Creative writing task: Write a short 3-sentence story about a robot learning to paint.\n",
    "Build the story one sentence at a time.\"\"\"\n",
    "\n",
    "async def tot_bfs(problem, namespace, branching=3, depth=3, keep_top_k=2):\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/tot\")\n",
    "\n",
    "    # Initial states: just the problem\n",
    "    states = [\"\"]\n",
    "\n",
    "    for d in range(depth):\n",
    "        print(f\"\\n=== Depth {d+1} ===\")\n",
    "        candidates = []\n",
    "\n",
    "        # Generate candidates for each state\n",
    "        for state in states:\n",
    "            prompt = f\"{problem}\\n\\nStory so far: {state if state else '(empty)'}\\n\\nWrite the next sentence only:\"\n",
    "\n",
    "            # Generate multiple candidates\n",
    "            gen_tasks = []\n",
    "            for _ in range(branching):\n",
    "                gen_tasks.append(\n",
    "                    client.chat.completions.create(\n",
    "                        model=\"gpt-4.1-nano\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    )\n",
    "                )\n",
    "            responses = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "            for resp in responses:\n",
    "                next_sentence = resp.choices[0].message.content.strip()\n",
    "                new_state = f\"{state} {next_sentence}\".strip()\n",
    "                candidates.append(new_state)\n",
    "\n",
    "        # Evaluate candidates\n",
    "        scores = []\n",
    "        eval_tasks = []\n",
    "        for cand in candidates:\n",
    "            eval_prompt = (\n",
    "                f\"{problem}\\n\\nPartial story: {cand}\\n\\n\"\n",
    "                f\"Rate this partial story from 1-10 for creativity and coherence. \"\n",
    "                f\"Reply with ONLY a number.\"\n",
    "            )\n",
    "            eval_tasks.append(\n",
    "                client.chat.completions.create(\n",
    "                    model=\"gpt-4.1-nano\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "                )\n",
    "            )\n",
    "        eval_responses = await asyncio.gather(*eval_tasks)\n",
    "\n",
    "        for i, resp in enumerate(eval_responses):\n",
    "            try:\n",
    "                score = float(resp.choices[0].message.content.strip())\n",
    "            except ValueError:\n",
    "                score = 5.0  # default\n",
    "            scores.append(score)\n",
    "            print(f\"  Candidate (score={score}): {candidates[i][:100]}...\")\n",
    "\n",
    "        # Keep top-K\n",
    "        ranked = sorted(zip(scores, candidates), reverse=True)\n",
    "        states = [cand for _, cand in ranked[:keep_top_k]]\n",
    "        print(f\"  Kept top {keep_top_k}\")\n",
    "\n",
    "    return states[0]  # best story\n",
    "\n",
    "print(\"=== Tree-of-Thought BFS — Run 1 ===\")\n",
    "best_story = await tot_bfs(PROBLEM, namespace=\"tot_demo\")\n",
    "print(f\"\\nBest story:\\n{best_story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run — all cached, deterministic\n",
    "print(\"=== Tree-of-Thought BFS — Run 2 (cached) ===\")\n",
    "t0 = time.time()\n",
    "best_story_2 = await tot_bfs(PROBLEM, namespace=\"tot_demo\")\n",
    "print(f\"\\nBest story:\\n{best_story_2}\")\n",
    "print(f\"\\nTime: {time.time() - t0:.2f}s (all cached)\")\n",
    "print(f\"Same result? {best_story == best_story_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. RAG Pipeline A — Simple Retrieval + Generation\n",
    "\n",
    "A minimal RAG pipeline: retrieve relevant chunks from a knowledge base using keyword overlap, then generate an answer with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./cache/rag\", ignore_errors=True)\n",
    "\n",
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "import math\n",
    "\n",
    "# Simple knowledge base\n",
    "KNOWLEDGE_BASE = [\n",
    "    \"The Eiffel Tower is 330 meters tall and is located in Paris, France. It was built in 1889 for the World's Fair.\",\n",
    "    \"The Great Wall of China is over 21,000 kilometers long. It was built over many centuries starting from the 7th century BC.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight, water, and CO2 into glucose and oxygen.\",\n",
    "    \"The human heart beats approximately 100,000 times per day, pumping about 7,500 liters of blood.\",\n",
    "    \"Python was created by Guido van Rossum and first released in 1991. It emphasizes code readability.\",\n",
    "    \"The Pacific Ocean is the largest ocean, covering more than 165 million square kilometers.\",\n",
    "    \"DNA stands for deoxyribonucleic acid. It carries genetic instructions for the development of all living organisms.\",\n",
    "    \"The speed of light in a vacuum is approximately 299,792,458 meters per second.\",\n",
    "    \"Mount Everest is 8,849 meters tall, making it the highest point on Earth above sea level.\",\n",
    "    \"The Amazon Rainforest produces about 20% of the world's oxygen and is home to 10% of all species.\",\n",
    "]\n",
    "\n",
    "def simple_retrieve(query, knowledge_base, top_k=3):\n",
    "    \"\"\"Retrieve most relevant chunks using word overlap (bag-of-words).\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    scored = []\n",
    "    for chunk in knowledge_base:\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        overlap = len(query_words & chunk_words)\n",
    "        scored.append((overlap, chunk))\n",
    "    scored.sort(reverse=True)\n",
    "    return [chunk for _, chunk in scored[:top_k]]\n",
    "\n",
    "async def rag_pipeline_a(query, namespace):\n",
    "    \"\"\"Simple RAG: retrieve + generate.\"\"\"\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/rag\")\n",
    "\n",
    "    # Retrieve\n",
    "    retrieved = simple_retrieve(query, KNOWLEDGE_BASE)\n",
    "    context = \"\\n\".join(f\"- {chunk}\" for chunk in retrieved)\n",
    "\n",
    "    # Generate\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer the question based on the provided context. Be concise.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content, retrieved\n",
    "\n",
    "# Test queries\n",
    "rag_queries = [\n",
    "    \"How tall is the Eiffel Tower?\",\n",
    "    \"What is photosynthesis?\",\n",
    "    \"How fast is the speed of light?\",\n",
    "    \"Tell me about the Amazon Rainforest.\",\n",
    "    \"Who created Python?\",\n",
    "]\n",
    "\n",
    "print(\"=== RAG Pipeline A ===\")\n",
    "rag_a_results = []\n",
    "for q in rag_queries:\n",
    "    answer, chunks = await rag_pipeline_a(q, namespace=\"rag_a\")\n",
    "    rag_a_results.append({\"query\": q, \"answer\": answer})\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. RAG Pipeline B — Retrieve + Re-rank + Generate\n",
    "\n",
    "An enhanced pipeline that adds an LLM-based **re-ranking** step: retrieve more chunks, use the LLM to pick the most relevant ones, then generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag_pipeline_b(query, namespace):\n",
    "    \"\"\"Enhanced RAG: retrieve → re-rank (LLM) → generate.\"\"\"\n",
    "    client = AsyncOpenAI(namespace=namespace, cachedir=\"./cache/rag\")\n",
    "\n",
    "    # Retrieve more candidates\n",
    "    retrieved = simple_retrieve(query, KNOWLEDGE_BASE, top_k=5)\n",
    "    chunks_text = \"\\n\".join(f\"{i+1}. {chunk}\" for i, chunk in enumerate(retrieved))\n",
    "\n",
    "    # Re-rank with LLM\n",
    "    rerank_response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a relevance judge. Given a question and numbered passages, return ONLY the numbers of the top 2 most relevant passages, comma-separated.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nPassages:\\n{chunks_text}\"},\n",
    "        ],\n",
    "    )\n",
    "    rerank_text = rerank_response.choices[0].message.content\n",
    "\n",
    "    # Parse re-ranked indices\n",
    "    try:\n",
    "        indices = [int(x.strip()) - 1 for x in rerank_text.replace(\" \", \"\").split(\",\") if x.strip().isdigit()]\n",
    "        reranked = [retrieved[i] for i in indices if 0 <= i < len(retrieved)]\n",
    "    except (ValueError, IndexError):\n",
    "        reranked = retrieved[:2]  # fallback\n",
    "\n",
    "    if not reranked:\n",
    "        reranked = retrieved[:2]\n",
    "\n",
    "    context = \"\\n\".join(f\"- {chunk}\" for chunk in reranked)\n",
    "\n",
    "    # Generate\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer the question based on the provided context. Be concise.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content, reranked\n",
    "\n",
    "print(\"=== RAG Pipeline B (with re-ranking) ===\")\n",
    "rag_b_results = []\n",
    "for q in rag_queries:\n",
    "    answer, reranked = await rag_pipeline_b(q, namespace=\"rag_b\")\n",
    "    rag_b_results.append({\"query\": q, \"answer\": answer})\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5e. Benchmarking the two RAG pipelines\n",
    "\n",
    "Compare Pipeline A and Pipeline B on the same questions, then show that re-running the benchmark is instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Benchmark: Pipeline A vs Pipeline B ===\")\n",
    "print(f\"{'Query':<45} {'Pipeline A':<40} {'Pipeline B':<40}\")\n",
    "print(\"-\" * 125)\n",
    "for a, b in zip(rag_a_results, rag_b_results):\n",
    "    q = a[\"query\"]\n",
    "    ans_a = a[\"answer\"][:37] + \"...\" if len(a[\"answer\"]) > 40 else a[\"answer\"]\n",
    "    ans_b = b[\"answer\"][:37] + \"...\" if len(b[\"answer\"]) > 40 else b[\"answer\"]\n",
    "    print(f\"{q:<45} {ans_a:<40} {ans_b:<40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the full benchmark — all cached, near-instant\n",
    "print(\"=== Re-running full benchmark (all cached) ===\")\n",
    "t0 = time.time()\n",
    "\n",
    "rag_a_cached = []\n",
    "rag_b_cached = []\n",
    "for q in rag_queries:\n",
    "    ans_a, _ = await rag_pipeline_a(q, namespace=\"rag_a\")\n",
    "    ans_b, _ = await rag_pipeline_b(q, namespace=\"rag_b\")\n",
    "    rag_a_cached.append(ans_a)\n",
    "    rag_b_cached.append(ans_b)\n",
    "\n",
    "cache_time = time.time() - t0\n",
    "print(f\"Time for full benchmark (cached): {cache_time:.2f}s\")\n",
    "print(f\"Results identical to first run? A={[r['answer'] for r in rag_a_results] == rag_a_cached}, B={[r['answer'] for r in rag_b_results] == rag_b_cached}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "CacheSaver gives you:\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---|---|\n",
    "| **Transparent caching** | Never pay twice for the same LLM call |\n",
    "| **Namespace isolation** | Reproducible experiments across runs |\n",
    "| **Drop-in replacement** | Change one import, keep your code |\n",
    "| **Async-native** | Speed up experiments with `asyncio.gather` |\n",
    "| **Error recovery** | Resume from where you left off |\n",
    "| **Iterative development** | Modify pipelines without re-running cached steps |\n",
    "\n",
    "For more providers (Anthropic, Gemini, Together AI, Groq, vLLM, OpenRouter, HuggingFace), see the `providers_example.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CacheSaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
