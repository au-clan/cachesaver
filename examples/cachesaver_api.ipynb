{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CacheSaver-Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from diskcache import Cache\n",
    "from typing import List, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from together import AsyncTogether\n",
    "\n",
    "from cachesaver.pipelines import OnlineAPI\n",
    "from cachesaver.typedefs import BaseRequest, Batch, Response, SingleRequestModel, BatchRequestModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly an online LLM class has to be defined in the standards of CacheSaver. It's basically the last layer in Cachesaver's pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLLM(SingleRequestModel, BatchRequestModel):\n",
    "    def __init__(self, client: Any, model: str):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    async def request(self, request: BaseRequest) -> Response:\n",
    "        completion = await self.client.chat.completions.create(\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\" : \"user\",\n",
    "                    \"content\" : request.prompt\n",
    "                }\n",
    "            ],\n",
    "            model = self.model,\n",
    "            n = request.n,\n",
    "            max_tokens= request.max_completion_tokens or None, # or None not needed but just to be explicit\n",
    "            temperature = request.temperature or 1,\n",
    "            stop = request.stop or None,\n",
    "            top_p = request.top_p or 1,\n",
    "            seed = request.seed or None,\n",
    "            logprobs = request.logprobs or False,\n",
    "            top_logprobs = request.top_logprobs or None,\n",
    "        )\n",
    "        response = Response(\n",
    "            data = [choice.message.content for choice in completion.choices]\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    async def batch_request(self, batch: Batch) -> List[Response]:\n",
    "        requests = [self.request(request) for request in batch.requests]\n",
    "        completions = await asyncio.gather(*requests)\n",
    "        return completions\n",
    "\n",
    "\n",
    "\n",
    "client = AsyncTogether(api_key=os.environ.get('TOGETHER_API_KEY_PERS'))\n",
    "model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "model = OnlineLLM(client, model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a simple requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = BaseRequest(\n",
    "    prompt = \"What is the meaning of life?\",\n",
    "    n = 1,\n",
    "    request_id = \"sth1\",\n",
    "    namespace=\"sth\",\n",
    ")\n",
    "\n",
    "await model.request(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add more details regarding the model's decoding process you have to redefine the `Request` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Request(BaseRequest):\n",
    "    temperature: float=1\n",
    "    max_completion_tokens: int=None\n",
    "\n",
    "request = Request(\n",
    "    prompt = \"What is the meaning of life?\",\n",
    "    n = 1,\n",
    "    request_id = \"sth1\",\n",
    "    namespace=\"sth\",\n",
    "    max_completion_tokens = 30,\n",
    "    temperature = 0.5,\n",
    ")\n",
    "response = await model.request(request)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Response\n",
    "\n",
    "The `Response` object retains data and metadata of the whole cachesaver process.\n",
    "\n",
    "- `Response.data` holds whatever the LLM returned.\n",
    "- `Response.cached` includes whether the sample returned from the LLM was retrieved from the cache or not. That if in the pipeline `cachesaver.AsyncCacher` is included. Otherwise it defaults to `None`.\n",
    "- `Response.duplicated` includes whether the sample was duplicated or not. That if in the pipeline `cachesaver.AsyncDeduplicator` is included. Otherwise it defaults to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{response.data=}\\n\")\n",
    "print(f\"{response.cached=}\\n\")\n",
    "print(f\"{response.duplicated=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the model to the Cachesaver pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Request(Request):\n",
    "    temperature: float=1\n",
    "    max_completion_tokens: int=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_request_response=Response(data=['The meaning of life is a question that has puzzled', 'The meaning of life is a question that has puzzled'], cached=[True, True], duplicated=[False, True])\n",
      "\n",
      "pipeline_batch_response=[Response(data=['The meaning of life is a question that has puzzled', 'The question of the meaning of life has puzzled philosophers'], cached=[True, True], duplicated=[False, True]), Response(data=[\"As a neutral AI, I don't have personal\"], cached=[False], duplicated=[False])]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "request1 = Request(prompt=\"What is the meaning of life?\", n=2, request_id=\"sth1\", namespace=\"sth\", max_completion_tokens=10)\n",
    "request2 = Request(prompt=\"Cats or dogs?\", n=1, request_id=\"sth2\", namespace=\"sth\", max_completion_tokens=10)\n",
    "batch = Batch(requests=[request1, request2])\n",
    "\n",
    "cache = Cache(\"../caches/developping\")\n",
    "\n",
    "pipeline = OnlineAPI(\n",
    "    model = model,\n",
    "    cache=cache,\n",
    "    batch_size = 2,\n",
    "    timeout = 1\n",
    ")\n",
    "\n",
    "pipeline_request_response = await pipeline.request(request1)\n",
    "print(f\"{pipeline_request_response=}\\n\")\n",
    "\n",
    "pipeline_batch_response = await pipeline.batch_request(batch)\n",
    "print(f\"{pipeline_batch_response=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cachesaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
