{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# CacheSaver — Multi-Provider Examples\n",
    "\n",
    "This notebook shows how to use CacheSaver with each supported LLM provider.\n",
    "CacheSaver acts as a drop-in wrapper: you use the same API you already know,\n",
    "and caching, deduplication, and batching happen transparently behind the scenes.\n",
    "\n",
    "**Providers covered:**\n",
    "1. OpenAI\n",
    "2. Together AI\n",
    "3. Anthropic (Claude)\n",
    "4. Google Gemini\n",
    "5. Hugging Face\n",
    "6. vLLM\n",
    "7. OpenRouter\n",
    "8. Groq\n",
    "9. HF Transformers (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0b1c2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. OpenAI\n",
    "\n",
    "The OpenAI wrapper is a drop-in replacement for `openai.AsyncOpenAI`.\n",
    "It mirrors the `client.chat.completions.create(...)` interface exactly,\n",
    "including support for the `n` parameter to request multiple completions.\n",
    "\n",
    "Set the `OPENAI_API_KEY` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4f5a6",
   "metadata": {},
   "source": [
    "### Standard OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()  # uses OPENAI_API_KEY env var\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    n=3\n",
    ")\n",
    "print(\"OpenAI SDK:\", [c.message.content for c in response.choices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b3c4",
   "metadata": {},
   "source": [
    "### CacheSaver OpenAI Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(namespace=\"openai_demo\", cachedir=\"./cache\")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    n=3\n",
    ")\n",
    "print(\"CacheSaver OpenAI:\", [c.message.content for c in response.choices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same call again — results come from cache (no API call)\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    n=3\n",
    ")\n",
    "print(\"Cached:\", [c.message.content for c in response.choices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Together AI\n",
    "\n",
    "Together AI uses an OpenAI-compatible API, so the interface is identical.\n",
    "\n",
    "Set the `TOGETHER_API_KEY` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "### Standard Together SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import AsyncTogether\n",
    "\n",
    "client = AsyncTogether()  # uses TOGETHER_API_KEY env var\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Together SDK:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "### CacheSaver Together Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.together import AsyncTogether\n",
    "\n",
    "client = AsyncTogether(namespace=\"together_demo\", cachedir=\"./cache\")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"CacheSaver Together:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached call\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Cached:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Anthropic (Claude)\n",
    "\n",
    "The Anthropic wrapper mirrors the `client.messages.create(...)` interface.\n",
    "Claude does not support `n > 1` natively, so CacheSaver makes `n` separate\n",
    "API calls concurrently and returns them as a list.\n",
    "\n",
    "Set the `ANTHROPIC_API_KEY` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### Standard Anthropic SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "client = AsyncAnthropic()  # uses ANTHROPIC_API_KEY env var\n",
    "\n",
    "message = await client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=50,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Anthropic SDK:\", message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "### CacheSaver Anthropic Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.anthropic import AsyncAnthropic\n",
    "\n",
    "client = AsyncAnthropic(namespace=\"anthropic_demo\", cachedir=\"./cache\")\n",
    "\n",
    "# n=1: returns the Message object directly\n",
    "message = await client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=50,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"CacheSaver Anthropic:\", message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached call\n",
    "message = await client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=50,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Cached:\", message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n>1: CacheSaver makes 3 concurrent API calls and returns a list\n",
    "messages = await client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=50,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    n=3\n",
    ")\n",
    "print(\"CacheSaver Anthropic (n=3):\", [m.content[0].text for m in messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Google Gemini\n",
    "\n",
    "The Gemini wrapper mirrors the `client.models.generate_content(...)` interface\n",
    "from the `google-genai` SDK. Like Claude, `n > 1` is handled by making\n",
    "concurrent calls.\n",
    "\n",
    "Set the `GOOGLE_API_KEY` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0b1c3",
   "metadata": {},
   "source": [
    "### Standard Google GenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()  # uses GOOGLE_API_KEY env var\n",
    "\n",
    "response = await client.aio.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Name a random city (only the name).\",\n",
    ")\n",
    "print(\"Gemini SDK:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d9e1",
   "metadata": {},
   "source": [
    "### CacheSaver Gemini Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.gemini import AsyncGemini\n",
    "\n",
    "client = AsyncGemini(namespace=\"gemini_demo\", cachedir=\"./cache\")\n",
    "\n",
    "# n=1: returns the response object directly\n",
    "response = await client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Name a random pet (only the name).\",\n",
    ")\n",
    "print(\"CacheSaver Gemini:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached call\n",
    "client = AsyncGemini(namespace=\"gemini_demo\", cachedir=\"./cache\")\n",
    "\n",
    "response = await client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Name a random pet (only the name).\",\n",
    ")\n",
    "print(\"Cached:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n>1: CacheSaver makes 3 concurrent API calls and returns a list\n",
    "responses = await client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Name a random city (only the name).\",\n",
    "    n=3\n",
    ")\n",
    "print(\"CacheSaver Gemini (n=3):\", [c.content.parts[0].text for c in responses.candidates])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10bb20",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hugging Face\n",
    "\n",
    "The Hugging Face wrapper uses the `huggingface_hub` Inference Client.\n",
    "It mirrors the OpenAI-compatible `client.chat.completions.create(...)` interface,\n",
    "and supports the `n` parameter.\n",
    "\n",
    "Set the `HF_TOKEN` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21cc31",
   "metadata": {},
   "source": [
    "### Standard Hugging Face SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc32dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace SDK: Perth.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "client = AsyncInferenceClient()  # uses HF_TOKEN env var\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_tokens=50,\n",
    "    n=2\n",
    ")\n",
    "print(\"HuggingFace SDK:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43ee53",
   "metadata": {},
   "source": [
    "### CacheSaver Hugging Face Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.huggingface import AsyncHuggingFace\n",
    "\n",
    "client = AsyncHuggingFace(namespace=\"hf_demo\", cachedir=\"./cache\")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_tokens=50,\n",
    ")\n",
    "print(\"CacheSaver HuggingFace:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached call\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_tokens=50,\n",
    ")\n",
    "print(\"Cached:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76bb86",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. vLLM\n",
    "\n",
    "vLLM exposes an OpenAI-compatible API server. The CacheSaver wrapper uses the\n",
    "OpenAI client under the hood, pointed at your vLLM server URL.\n",
    "No API key is required by default (defaults to `\"EMPTY\"`).\n",
    "\n",
    "Start a vLLM server first:\n",
    "```bash\n",
    "vllm serve meta-llama/Llama-3-8B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87cc97",
   "metadata": {},
   "source": [
    "### Standard vLLM Usage (via OpenAI SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"vLLM SDK:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9eeb9",
   "metadata": {},
   "source": [
    "### CacheSaver vLLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebaffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.vllm import AsyncVLLM\n",
    "\n",
    "client = AsyncVLLM(\n",
    "    namespace=\"vllm_demo\",\n",
    "    cachedir=\"./cache\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"CacheSaver vLLM:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached call\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Cached:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcbbdc",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. OpenRouter\n",
    "\n",
    "OpenRouter provides a unified API to 300+ models. It uses the OpenAI-compatible\n",
    "format, so the interface is identical to OpenAI. The `base_url` defaults to\n",
    "`https://openrouter.ai/api/v1` automatically.\n",
    "\n",
    "Set the `OPENROUTER_API_KEY` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbedcced",
   "metadata": {},
   "source": [
    "### Standard OpenRouter Usage (via OpenAI SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfeddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"OpenRouter SDK:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfeeeaf",
   "metadata": {},
   "source": [
    "### CacheSaver OpenRouter Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.openrouter import AsyncOpenRouter\n",
    "\n",
    "client = AsyncOpenRouter(\n",
    "    namespace=\"openrouter_demo\",\n",
    "    cachedir=\"./cache\",\n",
    "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"CacheSaver OpenRouter:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenRouter(\n",
    "    namespace=\"openrouter_demo\",\n",
    "    cachedir=\"./cache\",\n",
    "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Cached call\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Cached:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbbbdc",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Groq\n",
    "\n",
    "Groq provides ultra-fast inference. The wrapper mirrors the\n",
    "`client.chat.completions.create(...)` interface from the `groq` SDK.\n",
    "\n",
    "Set the `GROQ_API_KEY` environment variable before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcccde",
   "metadata": {},
   "source": [
    "### Standard Groq SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import AsyncGroq\n",
    "\n",
    "client = AsyncGroq()  # uses GROQ_API_KEY env var\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Groq SDK:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfeeeab",
   "metadata": {},
   "source": [
    "### CacheSaver Groq Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.groq import AsyncGroq\n",
    "\n",
    "client = AsyncGroq(namespace=\"groq_demo\", cachedir=\"./cache\")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"CacheSaver Groq:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaaade",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncGroq(namespace=\"groq_demo\", cachedir=\"./cache\")\n",
    "\n",
    "# Cached call\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Cached:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6xdfp65geeh",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. HF Transformers (Local Inference)\n",
    "\n",
    "The HF Transformers wrapper runs models locally on your own hardware using\n",
    "the `transformers` library. Unlike the cloud providers above, this uses the\n",
    "**LocalAPI** pipeline (Cache → Batcher → Model) optimized for GPU batching.\n",
    "\n",
    "No API key needed — just install the optional dependencies:\n",
    "```bash\n",
    "pip install cachesaver[transformers]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ezspmiq5os",
   "metadata": {},
   "source": [
    "### Async HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch45tkgef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.transformers import AsyncHFTransformers\n",
    "\n",
    "client = AsyncHFTransformers(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    namespace=\"hf_transformers_demo\",\n",
    "    cachedir=\"./cache\",\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "print(\"CacheSaver HF Transformers:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ava086q8s4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached call — no GPU inference needed\n",
    "response = await client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "print(\"Cached:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vx467yxey9r",
   "metadata": {},
   "source": [
    "### Sync HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r63td1oqcp",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.transformers import HFTransformers\n",
    "\n",
    "client = HFTransformers(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    namespace=\"hf_transformers_sync_demo\",\n",
    "    cachedir=\"./cache\",\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "print(\"Sync HF Transformers:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b3a32",
   "metadata": {},
   "source": [
    "---\n",
    "## Sync Usage\n",
    "\n",
    "Each provider also offers a synchronous wrapper for use outside of async contexts.\n",
    "These use `nest_asyncio` under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d98a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachesaver.models.openai import OpenAI\n",
    "from cachesaver.models.anthropic import Anthropic\n",
    "from cachesaver.models.gemini import Gemini\n",
    "from cachesaver.models.huggingface import HuggingFace\n",
    "from cachesaver.models.vllm import VLLM\n",
    "from cachesaver.models.openrouter import OpenRouter\n",
    "from cachesaver.models.groq import Groq\n",
    "from cachesaver.models.transformers import HFTransformers\n",
    "\n",
    "# Sync OpenAI\n",
    "client = OpenAI(namespace=\"sync_demo\", cachedir=\"./cache\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Sync OpenAI:\", response.choices[0].message.content)\n",
    "\n",
    "# Sync Anthropic\n",
    "client = Anthropic(namespace=\"sync_demo\", cachedir=\"./cache\")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=50,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Sync Anthropic:\", message.content[0].text)\n",
    "\n",
    "# Sync Gemini\n",
    "client = Gemini(namespace=\"sync_demo\", cachedir=\"./cache\")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Name a random city (only the name).\",\n",
    ")\n",
    "print(\"Sync Gemini:\", response.text)\n",
    "\n",
    "# Sync HuggingFace\n",
    "client = HuggingFace(namespace=\"sync_demo\", cachedir=\"./cache\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_tokens=50,\n",
    ")\n",
    "print(\"Sync HuggingFace:\", response.choices[0].message.content)\n",
    "\n",
    "# Sync vLLM\n",
    "client = VLLM(namespace=\"sync_demo\", cachedir=\"./cache\", base_url=\"http://localhost:8000/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Sync vLLM:\", response.choices[0].message.content)\n",
    "\n",
    "# Sync OpenRouter\n",
    "client = OpenRouter(namespace=\"sync_demo\", cachedir=\"./cache\", api_key=os.environ.get(\"OPENROUTER_API_KEY\"))\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Sync OpenRouter:\", response.choices[0].message.content)\n",
    "\n",
    "# Sync Groq\n",
    "client = Groq(namespace=\"sync_demo\", cachedir=\"./cache\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    ")\n",
    "print(\"Sync Groq:\", response.choices[0].message.content)\n",
    "\n",
    "# Sync HF Transformers\n",
    "client = HFTransformers(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    namespace=\"sync_demo\",\n",
    "    cachedir=\"./cache\",\n",
    "    batch_size=4,\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a random city (only the name).\"}],\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "print(\"Sync HF Transformers:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CacheSaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
