{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import asyncio\n",
    "from diskcache import Cache\n",
    "from groq import AsyncGroq\n",
    "from omegaconf import OmegaConf\n",
    "from cachesaver.pipelines import OnlineAPI\n",
    "from cachesaver.typedefs import Response, Batch\n",
    "from typing import Any, List\n",
    "\n",
    "from src.algorithms import *\n",
    "from src.models import OnlineLLM, API\n",
    "from src.typedefs import DecodingParameters, Model\n",
    "from src.tasks.game24 import EnvironmentGame24, AgentBfsGame24, AgentAggregateGame24, AgentEvaluateGame24, StateGame24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import RateLimitError\n",
    "\n",
    "class MockLLM(Model):\n",
    "    def __init__(self, client: Any, model: str)-> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    async def request(self, prompt: str, n: int, request_id: int, namespace: str, params: DecodingParameters) -> Response:\n",
    "        sleep = 1\n",
    "        while True:\n",
    "            try:\n",
    "                completion = await self.client.chat.completions.create(\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\" : \"user\",\n",
    "                            \"content\" : prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    model = self.model,\n",
    "                    n = n,\n",
    "                    max_tokens= params.max_completion_tokens or None, # or None not needed but just to be explicit\n",
    "                    temperature = params.temperature or 1,\n",
    "                    stop = params.stop or None,\n",
    "                    top_p = params.top_p or 1,\n",
    "                    seed = 1234,\n",
    "                    logprobs = params.logprobs or False,\n",
    "                    top_logprobs = None,\n",
    "                )\n",
    "                break\n",
    "            except RateLimitError as e:\n",
    "                await asyncio.sleep(max(sleep, 90))\n",
    "                sleep *= 2\n",
    "            except Exception as e:\n",
    "                print(f\"Error {e}\")\n",
    "                raise e\n",
    "        input_tokens = completion.usage.prompt_tokens\n",
    "        completion_tokens = completion.usage.completion_tokens\n",
    "        response = [choice.message.content for choice in completion.choices]\n",
    "        return response\n",
    "    \n",
    "    async def batch_request(self, batch: Batch) -> List[Response]:\n",
    "        requests = [self.request(request) for request in batch.requests]\n",
    "        completions = await asyncio.gather(*requests)\n",
    "        return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = \"llama-3.3-70b-versatile\"\n",
    "game_simple = \"10 10 1 4\"\n",
    "\n",
    "cache = Cache(f\"caches\\\\game24\")\n",
    "\n",
    "client = AsyncGroq()\n",
    "model = MockLLM(client=client, model=llm)\n",
    "# pipeline = OnlineAPI(\n",
    "#     model=model,\n",
    "#     cache=cache,\n",
    "#     batch_size=2,\n",
    "#     timeout=0.1,\n",
    "# )\n",
    "# api = API(\n",
    "#     pipeline=pipeline,\n",
    "#     model=llm\n",
    "# )\n",
    "\n",
    "state = StateGame24(\n",
    "    puzzle=game_simple,\n",
    "    current_state=game_simple,\n",
    "    steps=[],\n",
    "    randomness=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StateGame24(puzzle='10 10 1 4', current_state='(10 * 1) + (10 + 4)', steps=['10 + 4 = 14 (left: 10 1 14)', '10 * 1 = 10 (left: 10 14)', '10 + 14 = 24 (left: 24)', '(10 * 1) + (10 + 4)'], randomness=1121)]\n",
      "True 1.0\n"
     ]
    }
   ],
   "source": [
    "params = DecodingParameters(\n",
    "    temperature=0.7,\n",
    "    max_completion_tokens=100,\n",
    "    top_p=1.0,\n",
    "    stop=None,\n",
    "    logprobs=False,\n",
    ")\n",
    "\n",
    "config = OmegaConf.load(\"scripts\\\\game24.yaml\")\n",
    "\n",
    "agents = AgentDictGOT(\n",
    "    step=AgentBfsGame24,\n",
    "    aggregate=AgentAggregateGame24,\n",
    "    evaluate=AgentEvaluateGame24,\n",
    "    step_params=params,\n",
    "    aggregate_params=params,\n",
    "    eval_params=params,\n",
    ")\n",
    "method = AlgorithmGOT(\n",
    "    model=model,\n",
    "    agents=agents,\n",
    "    env=EnvironmentGame24,\n",
    "    num_selections=config.got.num_selections,\n",
    "    num_steps=config.got.num_steps,\n",
    "    num_best=config.got.num_best,\n",
    "    num_evaluations=config.got.num_evaluations,\n",
    ")\n",
    "\n",
    "result = await method.solve(idx=0, state=state, namespace=\"small\", value_cache=None)\n",
    "print(result)\n",
    "finished, correct = EnvironmentGame24.evaluate(result[0])\n",
    "print(finished, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet of each method, to test them each and individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10 + 10 = 20 (left: 20 1 4)', '10 + 1 = 11 (left: 10 11 4)', '10 + 4 = 14 (left: 10 14 1)', '10 * 10 = 100 (left: 100 1 4)', '10 * 1 = 10 (left: 10 10 4)']\n",
      "['10 + 10 = 20 (left: 20 1 4)', '10 + 4 = 14 (left: 10 14 1)', '10 * 1 = 10 (left: 10 10 4)']\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='20 1 4', steps=['10 + 10 = 20 (left: 20 1 4)'], randomness=5723), StateGame24(puzzle='10 10 1 4', current_state='10 14 1', steps=['10 + 4 = 14 (left: 10 14 1)'], randomness=6963), StateGame24(puzzle='10 10 1 4', current_state='10 10 4', steps=['10 * 1 = 10 (left: 10 10 4)'], randomness=5602)]\n",
      "[20.0, 20.0, 20.0]\n"
     ]
    }
   ],
   "source": [
    "generate = AgentBfsGame24()\n",
    "aggregate = AgentAggregateGame24()\n",
    "evaluate = AgentEvaluateGame24()\n",
    "env = EnvironmentGame24()\n",
    "\n",
    "generate_results = await generate.act(model=model, state=state, namespace=\"small\", request_id=0, params=params)\n",
    "print(generate_results)\n",
    "\n",
    "aggregate_results = await aggregate.act(model=model, state=state, actions=generate_results, k=3, n=1, namespace=\"small\", request_id=0, params=params)\n",
    "print(aggregate_results)\n",
    "\n",
    "proposals = []\n",
    "for action in aggregate_results:\n",
    "    proposals.append(env.step(state, action))\n",
    "\n",
    "print(proposals)\n",
    "\n",
    "evaluate_coroutines = [evaluate.act(model=model, state=state, n=1, namespace=\"small\", request_id=0, params=params, cache=None) for state in proposals]\n",
    "evaluate_results = await asyncio.gather(*evaluate_coroutines)\n",
    "print(evaluate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cachesaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
