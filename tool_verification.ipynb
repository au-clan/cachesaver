{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import asyncio\n",
    "from diskcache import Cache\n",
    "from groq import AsyncGroq\n",
    "from omegaconf import OmegaConf\n",
    "from cachesaver.pipelines import OnlineAPI\n",
    "from cachesaver.typedefs import Response, Batch\n",
    "from typing import Any, List\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from src.utils import tokens2cost\n",
    "from src.algorithms import *\n",
    "from src.models import OnlineLLM, API\n",
    "from src.typedefs import DecodingParameters, Model\n",
    "from src.tasks.game24 import EnvironmentGame24, AgentBfsGame24, AgentAggregateGame24, AgentEvaluateGame24, StateGame24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import RateLimitError\n",
    "\n",
    "class MockLLM(Model):\n",
    "    def __init__(self, client: Any, model: str)-> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    async def request(self, prompt: str, n: int, request_id: int, namespace: str, params: DecodingParameters) -> Response:\n",
    "        sleep = 1\n",
    "        while True:\n",
    "            try:\n",
    "                completion = await self.client.chat.completions.create(\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\" : \"user\",\n",
    "                            \"content\" : prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    model = self.model,\n",
    "                    n = n,\n",
    "                    max_tokens= params.max_completion_tokens or None, # or None not needed but just to be explicit\n",
    "                    temperature = params.temperature or 1,\n",
    "                    stop = params.stop or None,\n",
    "                    top_p = params.top_p or 1,\n",
    "                    seed = 1234,\n",
    "                    logprobs = params.logprobs or False,\n",
    "                    top_logprobs = None,\n",
    "                )\n",
    "                break\n",
    "            except RateLimitError as e:\n",
    "                await asyncio.sleep(max(sleep, 90))\n",
    "                sleep *= 2\n",
    "            except Exception as e:\n",
    "                print(f\"Error {e}\")\n",
    "                raise e\n",
    "        input_tokens = completion.usage.prompt_tokens\n",
    "        completion_tokens = completion.usage.completion_tokens\n",
    "        response = [choice.message.content for choice in completion.choices]\n",
    "        return response\n",
    "    \n",
    "    async def batch_request(self, batch: Batch) -> List[Response]:\n",
    "        requests = [self.request(request) for request in batch.requests]\n",
    "        completions = await asyncio.gather(*requests)\n",
    "        return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secret\n",
    "\n",
    "llm = \"llama-3.3-70b-versatile\"\n",
    "game_simple = \"10 10 1 4\"\n",
    "\n",
    "cache = Cache(f\"caches/game24\")\n",
    "\n",
    "client = AsyncGroq(api_key=secret.GROQ_API_KEYS[1])\n",
    "model = MockLLM(client=client, model=llm)\n",
    "# pipeline = OnlineAPI(\n",
    "#     model=model,\n",
    "#     cache=cache,\n",
    "#     batch_size=2,\n",
    "#     timeout=0.1,\n",
    "# )\n",
    "# api = API(\n",
    "#     pipeline=pipeline,\n",
    "#     model=llm\n",
    "# )\n",
    "\n",
    "state = StateGame24(\n",
    "    puzzle=game_simple,\n",
    "    current_state=game_simple,\n",
    "    steps=[],\n",
    "    randomness=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10 + 10 = 20 (left: 1 4 20)', '10 + 1 = 11 (left: 10 4 11)', '10 + 4 = 14 (left: 10 1 14)', '10 * 1 = 10 (left: 10 4 10)']]\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='1 4 20', steps=['10 + 10 = 20 (left: 1 4 20)'], randomness=6905), StateGame24(puzzle='10 10 1 4', current_state='10 4 11', steps=['10 + 1 = 11 (left: 10 4 11)'], randomness=6905), StateGame24(puzzle='10 10 1 4', current_state='10 1 14', steps=['10 + 4 = 14 (left: 10 1 14)'], randomness=6905), StateGame24(puzzle='10 10 1 4', current_state='10 4 10', steps=['10 * 1 = 10 (left: 10 4 10)'], randomness=6905)]\n",
      "[1.0, 1.0, 0.0, 20.0]\n",
      "[['10 + 4 = 14 (left: 10 14)', '10 * 4 = 40 (left: 10 40)', '4 + 10 = 14 (left: 10 14)', '4 * 10 = 40 (left: 4 40)']]\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='10 14', steps=['10 * 1 = 10 (left: 10 4 10)', '10 + 4 = 14 (left: 10 14)'], randomness=1395), StateGame24(puzzle='10 10 1 4', current_state='10 40', steps=['10 * 1 = 10 (left: 10 4 10)', '10 * 4 = 40 (left: 10 40)'], randomness=1395), StateGame24(puzzle='10 10 1 4', current_state='10 14', steps=['10 * 1 = 10 (left: 10 4 10)', '4 + 10 = 14 (left: 10 14)'], randomness=1395), StateGame24(puzzle='10 10 1 4', current_state='4 40', steps=['10 * 1 = 10 (left: 10 4 10)', '4 * 10 = 40 (left: 4 40)'], randomness=1395)]\n",
      "[20.0, 0.0, 20.0, 0.001]\n",
      "[['10 + 14 = 24 (left: 24)']]\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='24', steps=['10 * 1 = 10 (left: 10 4 10)', '10 + 4 = 14 (left: 10 14)', '10 + 14 = 24 (left: 24)'], randomness=1690)]\n",
      "[20.0]\n",
      "[['(10 * 1) + (10 + 4)']]\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='(10 * 1) + (10 + 4)', steps=['10 * 1 = 10 (left: 10 4 10)', '10 + 4 = 14 (left: 10 14)', '10 + 14 = 24 (left: 24)', '(10 * 1) + (10 + 4)'], randomness=1121)]\n",
      "[0.001]\n"
     ]
    }
   ],
   "source": [
    "params = DecodingParameters(\n",
    "    temperature=0.7,\n",
    "    max_completion_tokens=100,\n",
    "    top_p=1.0,\n",
    "    stop=None,\n",
    "    logprobs=False,\n",
    ")\n",
    "\n",
    "config = OmegaConf.load(\"scripts/game24.yaml\")\n",
    "\n",
    "agents = AgentDictGOT(\n",
    "    step=AgentBfsGame24,\n",
    "    aggregate=AgentAggregateGame24,\n",
    "    evaluate=AgentEvaluateGame24,\n",
    "    step_params=params,\n",
    "    aggregate_params=params,\n",
    "    eval_params=params,\n",
    ")\n",
    "method = AlgorithmGOT(\n",
    "    model=model,\n",
    "    agents=agents,\n",
    "    env=EnvironmentGame24,\n",
    "    num_selections=config.got.num_selections,\n",
    "    num_steps=config.got.num_steps,\n",
    "    num_best=config.got.num_best,\n",
    "    num_evaluations=config.got.num_evaluations,\n",
    ")\n",
    "\n",
    "results = await method.solve(idx=0, state=state, namespace=\"small\", value_cache=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet of each method, to test them each and individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10 + 10 = 20 (left: 1 4 20)', '10 + 1 = 11 (left: 10 4 11)', '10 + 4 = 14 (left: 10 1 14)', '10 * 10 = 100 (left: 1 4 100)', '10 * 1 = 10 (left: 10 4 10)']\n",
      "['10 + 10 = 20 (left: 1 4 20)', '10 + 4 = 14 (left: 10 1 14)', '10 * 1 = 10 (left: 10 4 10)']\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='1 4 20', steps=['10 + 10 = 20 (left: 1 4 20)'], randomness=4062), StateGame24(puzzle='10 10 1 4', current_state='10 1 14', steps=['10 + 4 = 14 (left: 10 1 14)'], randomness=9289), StateGame24(puzzle='10 10 1 4', current_state='10 4 10', steps=['10 * 1 = 10 (left: 10 4 10)'], randomness=7862)]\n",
      "[1.0, 20.0, 20.0]\n"
     ]
    }
   ],
   "source": [
    "generate = AgentBfsGame24()\n",
    "aggregate = AgentAggregateGame24()\n",
    "evaluate = AgentEvaluateGame24()\n",
    "env = EnvironmentGame24()\n",
    "\n",
    "generate_results = await generate.act(model=model, state=state, namespace=\"small\", request_id=0, params=params)\n",
    "print(generate_results)\n",
    "\n",
    "# proposals = []\n",
    "# for action in generate_results:\n",
    "#     proposals.append(env.step(state, action))\n",
    "\n",
    "aggregate_results = await aggregate.act(model=model, state=state, actions=generate_results, k=3, n=1, namespace=\"small\", request_id=0, params=params)\n",
    "print(aggregate_results)\n",
    "\n",
    "proposals = []\n",
    "for action in aggregate_results:\n",
    "    proposals.append(env.step(state, action))\n",
    "\n",
    "print(proposals)\n",
    "\n",
    "evaluate_coroutines = [evaluate.act(model=model, state=state, n=1, namespace=\"small\", request_id=0, params=params, cache=None) for state in proposals]\n",
    "evaluate_results = await asyncio.gather(*evaluate_coroutines)\n",
    "print(evaluate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10 + 14 = 24 (left: 24)', '10 * 14 = 140 (left: 140)', '14 - 10 = 4 (left: 4)', '14 / 10 = 1.4 (left: 1.4)', '10 - 14 = -4 (left: -4)', '10 / 14 = 0.714 (left: 0.714)']\n",
      "['10 + 14 = 24 (left: 24)']\n",
      "[StateGame24(puzzle='10 10 1 4', current_state='24', steps=['10 + 14 = 24 (left: 24)'], randomness=8809)]\n",
      "[20.0]\n"
     ]
    }
   ],
   "source": [
    "generate = AgentBfsGame24()\n",
    "aggregate = AgentAggregateGame24()\n",
    "evaluate = AgentEvaluateGame24()\n",
    "env = EnvironmentGame24()\n",
    "\n",
    "state = StateGame24(\n",
    "    puzzle=game_simple,\n",
    "    current_state=\"10 14\",\n",
    "    steps=[],\n",
    "    randomness=None,\n",
    ")\n",
    "\n",
    "generate_results = await generate.act(model=model, state=state, namespace=\"small\", request_id=0, params=params)\n",
    "print(generate_results)\n",
    "\n",
    "aggregate_results = await aggregate.act(model=model, state=state, actions=generate_results, k=3, n=1, namespace=\"small\", request_id=0, params=params)\n",
    "print(aggregate_results)\n",
    "\n",
    "proposals = []\n",
    "for action in aggregate_results:\n",
    "    proposals.append(env.step(state, action))\n",
    "\n",
    "print(proposals)\n",
    "\n",
    "evaluate_coroutines = [evaluate.act(model=model, state=state, n=1, namespace=\"small\", request_id=0, params=params, cache=None) for state in proposals]\n",
    "evaluate_results = await asyncio.gather(*evaluate_coroutines)\n",
    "print(evaluate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
