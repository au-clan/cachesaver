# CacheSaver
*Client-Side Framework for Efficient & Reproducible LLM Inference*


[![PyPI - Version](https://img.shields.io/pypi/v/cachesaver?logo=pypi&logoColor=white)](https://pypi.org/project/cachesaver/)
[![Paper](https://img.shields.io/badge/Arxiv-Paper-B31B1B?logo=arxiv)](https://openreview.net/forum?id=Ve2r5Bap1Q)

![CacheSaver Overview](assets/overview.png)
---

## What is CacheSaver?

**CacheSaver** is a lightweight client-side library that wraps existing LLM inference clients to make them:

- **Efficient** â€” repeated prompts and sub-problems are automatically cached and reused.  
- **Reproducible** â€” identical inputs yield identical outputs across runs.  
- **Compatible** â€” works with any LLM client (OpenAI, HuggingFace, vLLM, etc.) without modifying your code.

This repository accompanies our paper *â€œCacheSaver: Client-Side Caching for Efficient and Reproducible LLM Inferenceâ€*,  accepted at **EMNLP 2025**, and the related [project blog](https://au-clan.github.io/2025-06-21-cachesaver/).


---

## ğŸ’¡ Why CacheSaver?

### âš™ï¸ The Problem

- ğŸ§® LLM **inference dominates cost and energy consumption** sometimes up to **90% of the modelâ€™s total lifecycle**.  
- ğŸ¤” Many reasoning workflows (e.g., multi-agent, Tree-of-Thoughts, self-refinement) **reuse sub-problems** that are recomputed each time.  
- ğŸ² **Reproducibility** is tricky because most LLM APIs donâ€™t support deterministic seeding.

### ğŸš€ Our Solution

**CacheSaver** tackles these challenges with a **client-side cache and namespace system**:

- ğŸ”Œ Wrap your existing client: no model or server changes required.  
- ğŸ§© Introduce **namespaces** that act like â€œseedsâ€:  
  - Within a namespace â†’ random sampling stays IID.  
  - Across namespaces â†’ identical prompts yield identical results.  
- â™»ï¸ Cache intermediate reasoning steps to **reuse** them across runs.

âœ¨ **Result:** Faster, cheaper, and reproducible inference â€” all with minimal effort.


---

## Key Features

- ğŸš€ **Plug-and-Play:** One-line integration.
- ğŸ” **Cache & Reuse:** Avoid recomputation of repeated sub-problems.  
- ğŸ§© **Namespace Control:** Fine-grained reproducibility without losing randomness.  
- âš™ï¸ **Universal Compatibility:** Works with any LLM API or local model.  
- ğŸ§  **Lightweight:** No server-side code, minimal memory overhead.

---
## Installation

Install the latest release with its minimal dependencies:

```zsh
pip install cachesaver
```

You can also install the latest version from the source:

```bash
# Install with test dependencies (Will also publish in pypi soon).
git clone https://github.com/au-clan/cachesaver-core.git
cd cachesaver-core
pip install -e ".[test]"

# Run tests to verify everything works
pytest test/ -v
```

---

## Quickstart

```python
from cachesaver.models.openai import AsyncOpenAI

client = AsyncOpenAI(batch_size=2)

resp = await client.chat.completions.create(
    model="gpt-4.1-nano",
    messages=[
        {"role": "user", "content": "What's the capital of France?"}
    ]
)
```

For more use cases and examples of CacheSaver in action, please see the following [Jupyter notebook](https://github.com/au-clan/cachesaver/blob/main/examples.ipynb).
---

## Performance

![Applications](assets/applications.png)

We tested **CacheSaver** on common LLM research tasks such as hyperparameter tuning, ablation studies, and benchmarking, where many prompts or reasoning steps repeat across runs. By caching and reusing identical sub-queries on the client side, CacheSaver avoided redundant computations and achieved substantial savings, making experiments up to **6Ã— cheaper and 7Ã— faster** for hyperparameter tuning, **2.5Ã— cheaper** for ablation studies, and about **2Ã— cheaper** for benchmarking, all **without changing model behavior or code**.

---

## Feedback

We welcome all forms of feedback! Please open an issue for bugs, questions, or suggestions. Your input helps us improve CacheSaver, address common challenges efficiently, and build a stronger, more collaborative community.

---

## Citation

Official BibTeX to be announced with EMNLP 2025.