experiment_name: experiment1_30

# data: test2_hotpotQA
data: experiment1_hotpotqa

cachesaver_config:
  model_name: 'gpt-5-nano'
  batch_size: 20
  timeout: 2
  allow_batch_overflow: 1
  correctness: 0

client_kwargs:
  #model_name: 'gpt-5-nano'
  n: 1
  decoding_params:
    temperature: 1.0
    max_completion_tokens: 10_000
    top_p: 1.0

query_augmentation:
  # Options: pass, normalize, synonym_extension, rewriting, multi_query, hyde, decompose
  # component: [normalize, multi_query]
  # component: [rewriting]
  # component: [normalize, rewriting]
  # component: [decompose]
  component: [normalize, decompose]
  kwargs:
    nr_synonyms: 3
    lowercase: 1
    stop_word: 0

retriever:
  # Options: sparse, dot_product, cosine_similarity, l2
  type: [sparse, l2]
  kwargs:
    k: [6, 6]
    # should this be taken from the 'data' folder?
    embedding_model: sentence-transformers/all-MiniLM-L6-v2

context_builder:
  # Options: concat, cross_encoder
  component: concat
  kwargs: 
    k_context_builder: 6
    cross_enc_model_name: cross-encoder/ms-marco-MiniLM-L-6-v2 

