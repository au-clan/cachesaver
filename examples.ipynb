{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686cc4c",
   "metadata": {},
   "source": [
    "# CacheSaver Examples\n",
    "\n",
    "*üß† Exploring CacheSaver: Smarter Caching for LLM Workflows*\n",
    "\n",
    "In this notebook, we explore **CacheSaver**. CacheSaver lets you **reuse previous results**, **reduce redundant computation**, and **ensure reproducibility** across repeated runs with minimal setup.\n",
    "\n",
    "We will show different examples and use cases demonstrating how CacheSaver speeds up repeated or overlapping reasoning steps, makes results deterministic within namespaces, and works seamlessly with any existing LLM client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f368a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7a427",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è The first universal client-side optimization framework\n",
    "\n",
    "In this section, we will demonstrate how **CacheSaver** works seamlessly across different LLM providers.  \n",
    "We will run the **same prompt** using three backends ‚Äî **OpenAI**, **vLLM**, and **Together AI** ‚Äî first without CacheSaver, and then with it enabled.\n",
    "\n",
    "This experiment shows that CacheSaver‚Äôs caching and deterministic behavior are **backend-agnostic**: it can wrap any compatible client with no changes to your core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write one short proverb about patience (only the proverb, no explanation).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b6c6b",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without CacheSaver\n",
    "from openai import AsyncOpenAI                   # Without CacheSaver!\n",
    "from cachesaver.models.openai import AsyncOpenAI # With CacheSaver!\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06d261",
   "metadata": {},
   "source": [
    "### TogetherAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9903338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import AsyncTogether                   # Without CacheSaver!\n",
    "from cachesaver.models.together import AsyncTogether # With CacheSaver!\n",
    "\n",
    "client = AsyncTogether()\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct-Turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9dff94",
   "metadata": {},
   "source": [
    "### vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edea6f",
   "metadata": {},
   "source": [
    "To use CacheSaver with vLLM **or any other service that supports the OpenAI API Protocol**,  you can simply use the same `base_url` argument that you would use with the regular OpenAI client. CacheSaver is fully compatible with any OpenAI-style API endpoint, so no extra configuration is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23177731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without CacheSaver\n",
    "from openai import AsyncOpenAI                   # Without CacheSaver!\n",
    "from cachesaver.models.openai import AsyncOpenAI # With CacheSaver!\n",
    "\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=openai_api_key, \n",
    "    api_base=openai_api_base\n",
    ")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"openai-community/gpt2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4135a",
   "metadata": {},
   "source": [
    "## üå± Bringing the power of seeding to LLM inference\n",
    "\n",
    "In this section, we compare **vanilla OpenAI inference** with **CacheSaver-wrapped inference** to illustrate how caching and deterministic namespaces work in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f6509",
   "metadata": {},
   "source": [
    "### Example without CacheSaver\n",
    "Without CacheSaver, each call to `AsyncOpenAI()` produces genuinely random samples, even if the prompt and model are identical. As you vary `n` (the number of completions), you will see new random cities each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking for 3 output samples:  ['Kyoto', 'Ljubljana', 'Lagos']\n",
      "Asking for 2 output samples:  ['Lagos', 'Cairo']\n",
      "Asking for 5 output samples:  ['Lagos', 'Lagos', 'Lagos', 'Lagos', 'Kyoto']\n"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "prompt = \"Give me the name of a random city from all over the world (only the name, no other text).\"\n",
    "for n in [3, 2, 5]:\n",
    "    client = AsyncOpenAI()\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        n=n\n",
    "    )\n",
    "    print(f\"Asking for {n} output samples: \", [choice.message.content for choice in response.choices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbcff36",
   "metadata": {},
   "source": [
    "### Example with CacheSaver\n",
    "With CacheSaver, however, the same prompt and namespace always yield identical results, regardless of the sampling count or run order. CacheSaver acts as a **reproducible seeding layer** that remembers previous outputs, so repeated inference becomes deterministic and cache-efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking for 3 output samples:  ['Bogot√°', 'Lagos', 'Kyoto']\n",
      "Asking for 2 output samples:  ['Bogot√°', 'Lagos']\n",
      "Asking for 5 output samples:  ['Bogot√°', 'Lagos', 'Kyoto', 'Kyoto', 'Lagos']\n"
     ]
    }
   ],
   "source": [
    "from cachesaver.models.openai import AsyncOpenAI\n",
    "\n",
    "prompt = \"Give me the name of a random city from all over the world (only the name, no other text).\"\n",
    "for n in [3, 2, 5]:\n",
    "    client = AsyncOpenAI()\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        n=n\n",
    "    )\n",
    "    print(f\"Asking for {n} output samples: \", [choice.message.content for choice in response.choices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
